{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_variables\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradchek takes a tuple of tensor as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "#input = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True))\n",
    "input = Variable(torch.randn(20,20).double(), requires_grad=True)\n",
    "weight = Variable(torch.randn(30,20).double(), requires_grad=True)\n",
    "test = torch.autograd.gradcheck(LinearFunction.apply, (input, weight), raise_exception=False)\n",
    "#test = gradcheck(LinearFunction.apply, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters can never be volatile and, different than Variables,\n",
    "        # they require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.2661  0.1846  0.2349\n",
       "-0.0276 -0.2423  0.1566\n",
       " 0.1213 -0.2678  0.3188\n",
       "-0.2172  0.0197 -0.1826\n",
       "-0.3477 -0.0759  0.0739\n",
       "-0.0835 -0.4454  0.2019\n",
       "-0.5255  0.0957  0.3214\n",
       " 0.3367 -0.5961 -0.0632\n",
       "-0.0468  0.3601 -0.1715\n",
       " 0.4400 -0.6126  0.0687\n",
       " 0.1707  0.1458  0.0840\n",
       "-0.0321 -0.6874 -0.0445\n",
       "-0.6224 -0.3587  0.3448\n",
       "-0.0058  0.0234  0.0307\n",
       "-0.1892  0.0536  0.2159\n",
       " 0.1473  0.1312  0.0178\n",
       "-0.4800  0.0551  0.4445\n",
       " 0.0812 -0.2431  0.2469\n",
       "-0.0938  0.0541  0.2484\n",
       " 0.0072 -0.0870 -0.0755\n",
       "-0.0036 -0.1731 -0.0166\n",
       "-0.2493 -0.4714 -0.1641\n",
       "-0.3201  0.2340  0.3886\n",
       "-0.2374  0.0968  0.0951\n",
       " 0.3261  0.3388 -0.0971\n",
       "-0.1316  0.2211  0.1831\n",
       "-0.0044 -0.2310 -0.0230\n",
       "-0.0760 -0.0983 -0.1503\n",
       " 0.2486 -0.2221  0.0081\n",
       "-0.3169  0.0994  0.2958\n",
       " 0.3289 -0.4454  0.1390\n",
       "-0.1853  0.0086  0.3263\n",
       "-0.5287  0.0671  0.0712\n",
       "-0.2075 -0.1085  0.6731\n",
       " 0.2755 -0.1900 -0.2789\n",
       " 0.0627  0.1102 -0.0225\n",
       "-0.4621  0.3226  0.2127\n",
       "-0.1452  0.0555  0.3818\n",
       "-0.2450  0.0927  0.3577\n",
       " 0.2286 -0.3060 -0.1668\n",
       " 0.1853  0.2147 -0.1440\n",
       "-0.4388 -0.1749  0.1549\n",
       " 0.0349 -0.0887 -0.0410\n",
       " 0.0819 -0.1954 -0.0464\n",
       "-0.2634  0.1099  0.1876\n",
       " 0.0387 -0.5363  0.1418\n",
       " 0.0482 -0.3467  0.2523\n",
       "-0.2356 -0.2001  0.3930\n",
       " 0.0671 -0.2036 -0.3058\n",
       "-0.0779 -0.6227  0.2919\n",
       "-0.0134  0.2046  0.0183\n",
       "-0.1529 -0.1553 -0.0021\n",
       "-0.1252  0.3442 -0.0723\n",
       "-0.3893  0.4772 -0.2757\n",
       " 0.3152 -0.4073 -0.0926\n",
       "-0.2831  0.0351  0.1031\n",
       "-0.4351 -0.1263 -0.2084\n",
       " 0.0197  0.1415 -0.0337\n",
       "-0.2805 -0.4163  0.2728\n",
       " 0.0147  0.3241  0.0773\n",
       " 0.2002 -0.6268 -0.5422\n",
       "-0.3961 -0.1210  0.6926\n",
       " 0.4622 -0.2224 -0.0689\n",
       "-0.0591 -0.0122 -0.2876\n",
       " 0.0816 -0.3146  0.2320\n",
       " 0.0693 -0.0731 -0.5138\n",
       "-0.1317 -0.4215 -0.2683\n",
       "-0.1851 -0.0663 -0.1299\n",
       "-0.1720 -0.0093  0.1064\n",
       "-0.1532 -0.0518 -0.1676\n",
       "-0.1630  0.2042 -0.1499\n",
       "-0.3306  0.1181  0.3267\n",
       "-0.0971 -0.0435  0.0223\n",
       " 0.3168  0.0075  0.0820\n",
       "-0.1630 -0.1364  0.0586\n",
       " 0.3552 -0.5860 -0.4679\n",
       " 0.0327  0.1495  0.1302\n",
       " 0.1049 -0.4545  0.1006\n",
       " 0.2415 -0.0298 -0.1834\n",
       "-0.0109 -0.9131  0.2557\n",
       " 0.1027  0.0304  0.0419\n",
       " 0.1856 -0.0303 -0.0213\n",
       "-0.5014  0.3359  0.3244\n",
       " 0.0329 -0.1079 -0.2566\n",
       " 0.0795  0.0328 -0.3829\n",
       " 0.1672 -0.0495 -0.0340\n",
       "-0.2602 -0.1581 -0.1439\n",
       " 0.3348 -0.2761  0.1559\n",
       "-0.4943  0.0367  0.2538\n",
       " 0.4410  0.2597 -0.3609\n",
       "-0.3110 -0.0520  0.2029\n",
       "-0.2235  0.3585  0.2015\n",
       "-0.1314  0.3509  0.1407\n",
       "-0.0743 -0.3500 -0.1803\n",
       " 0.0250 -0.2594 -0.0583\n",
       " 0.0482 -0.6982  0.1577\n",
       "-0.2299 -0.1463  0.1517\n",
       " 0.0311 -0.5806 -0.1573\n",
       "-0.3130  0.0068  0.1979\n",
       "-0.3742 -0.2506 -0.0507\n",
       "-0.2064 -0.4823 -0.2094\n",
       " 0.1130 -0.3394 -0.0762\n",
       " 0.1568 -0.1427 -0.1643\n",
       " 0.1200  0.0734 -0.3062\n",
       "-0.2858 -0.2560  0.2339\n",
       " 0.1065 -0.6288  0.1663\n",
       " 0.0015  0.0844 -0.3170\n",
       "-0.0149  0.2086 -0.0549\n",
       " 0.1877 -0.5956 -0.2425\n",
       " 0.0284 -0.4147 -0.1955\n",
       " 0.0738 -0.4578 -0.1000\n",
       "-0.2568 -0.3168  0.2569\n",
       "-0.5617  0.0563  0.3438\n",
       " 0.1759 -0.4246 -0.4496\n",
       "-0.1669  0.2015 -0.1248\n",
       "-0.3647 -0.1735  0.4049\n",
       " 0.0286  0.0438 -0.1056\n",
       "-0.1234 -0.3127  0.1911\n",
       "-0.1929 -0.1110  0.3160\n",
       "-0.0912 -0.6121  0.1258\n",
       " 0.0335 -0.1442  0.1042\n",
       "-0.0158  0.3223 -0.0386\n",
       "-0.0684 -0.3341  0.1122\n",
       " 0.4121 -0.1125 -0.2887\n",
       "-0.6178  0.0940  0.3394\n",
       "-0.0179 -0.3224  0.0164\n",
       "-0.0736 -0.4249  0.2291\n",
       "-0.6227 -0.1657  0.5432\n",
       "[torch.FloatTensor of size 128x3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyLinear = Linear(20, 3)\n",
    "input = Variable(torch.randn(128, 20))\n",
    "output = MyLinear(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class NLLProbTargetFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, target, logsoftmax):\n",
    "        ctx.save_for_backward(target, logsoftmax)\n",
    "        output = -torch.mul(target,logsoftmax).sum(dim=1)\n",
    "        output = output.mean(dim=0)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        target, logsoftmax = ctx.saved_variables\n",
    "        length = len(target)\n",
    "        grad_target = grad_logsoftmax =  None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_target = -(((logsoftmax/length).t() * grad_output).t())\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_logsoftmax = -(((target/length).t() * grad_output).t())\n",
    "        return grad_target, grad_logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradchek takes a tuple of tensor as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (Variable(torch.randn(5,3).double().cuda(), requires_grad=True), Variable(torch.randn(5,3).double().cuda(), requires_grad=True))\n",
    "test = gradcheck(NLLProbTargetFunction.apply, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLProbTarget(nn.Module):\n",
    "    \"\"\"\n",
    "    def __init__(self): #\n",
    "        super(NLLProbTarget, self).__init__()\n",
    "        self.target = target\n",
    "        self.logsoftmax = logsoftmax\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters can never be volatile and, different than Variables,\n",
    "        # they require gradients by default.\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "    \"\"\"\n",
    "    def forward(self, target, logsoftmax):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return NLLProbTargetFunction.apply(target, logsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(size, index):\n",
    "    \"\"\" Creates a matrix of one hot vectors.\n",
    "        ```\n",
    "        import torch\n",
    "        import torch_extras\n",
    "        setattr(torch, 'one_hot', torch_extras.one_hot)\n",
    "        size = (3, 3)\n",
    "        index = torch.LongTensor([2, 0, 1]).view(-1, 1)\n",
    "        torch.one_hot(size, index)\n",
    "        # [[0, 0, 1], [1, 0, 0], [0, 1, 0]]\n",
    "        ```\n",
    "    \"\"\"\n",
    "    mask = torch.FloatTensor(*size).fill_(0)\n",
    "    ones = 1\n",
    "    if isinstance(index, Variable):\n",
    "        ones = Variable(torch.FloatTensor(index.size()).fill_(1))\n",
    "        mask = Variable(mask, volatile=index.volatile)\n",
    "    ret = mask.scatter_(1, index, ones)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Variable(one_hot((5,10),torch.LongTensor([1,2,3,4,0]).view(-1, 1)), requires_grad=False)\n",
    "logsoftmax = Variable(torch.randn(5,10), requires_grad=True)\n",
    "Mynll = NLLProbTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     1     0     0     0     0     0     0     0\n",
       "    0     0     0     1     0     0     0     0     0     0\n",
       "    0     0     0     0     1     0     0     0     0     0\n",
       "    1     0     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1228  0.4064  0.0809  0.3064 -1.5306 -0.6408  0.0616  1.3632 -0.1728  1.5466\n",
       " 0.5716  0.0561 -1.2361 -0.9009  4.0136 -0.2435 -0.5009  0.3759 -0.6373 -1.4164\n",
       "-0.8670  1.2151 -0.8476  0.9116  0.2084  1.5593  0.5596 -0.2088 -0.2560 -2.5845\n",
       " 0.2858  2.4982  0.7395  0.1254 -1.7044  0.3866  0.3665  1.1763  0.1173  0.5844\n",
       "-0.2326  0.0557  0.6567 -0.7532  1.3934 -1.5826  2.9197 -0.2702 -1.9872 -0.3143\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.9004 -2.3712 -2.6968 -2.4712 -4.3083 -3.4184 -2.7160 -1.4144 -2.9504 -1.2310\n",
       "-3.5634 -4.0789 -5.3711 -5.0359 -0.1214 -4.3785 -4.6359 -3.7591 -4.7723 -5.5514\n",
       "-3.6462 -1.5641 -3.6268 -1.8676 -2.5707 -1.2199 -2.2196 -2.9880 -3.0352 -5.3637\n",
       "-2.9714 -0.7590 -2.5177 -3.1318 -4.9615 -2.8706 -2.8907 -2.0809 -3.1399 -2.6728\n",
       "-3.5877 -3.2995 -2.6984 -4.1083 -1.9617 -4.9378 -0.4354 -3.6254 -5.3424 -3.6695\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "logsoftmax = LogSoftmax(logsoftmax)\n",
    "logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.6318\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = Mynll(target, logsoftmax)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.6318\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll = torch.nn.NLLLoss()\n",
    "nll(logsoftmax, Variable(torch.LongTensor([1,2,3,4,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
